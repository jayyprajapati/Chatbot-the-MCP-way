from core.llama import Llama
from mcp_client import MCPClient
from core.tools import ToolManager


class Chat:
    def __init__(self, llama_service: Llama, clients: dict[str, MCPClient]):
        self.llama_service: Llama = llama_service
        self.clients: dict[str, MCPClient] = clients
        self.messages: list[dict] = []

    async def _process_query(self, query: str):
        self.messages.append({"role": "user", "content": query})

    async def run(
        self,
        query: str,
    ) -> str:
        final_text_response = ""

        await self._process_query(query)

        while True:
            response = self.llama_service.chat(
                messages=self.messages,
                tools=await ToolManager.get_all_tools(self.clients),
            )

            self.llama_service.add_assistant_message(self.messages, response.get("message", {}).get("content", ""))

            if response.get("stop_reason") == "tool_use":
                print(self.llama_service.text_from_message(response))
                tool_result_parts = await ToolManager.execute_tool_requests(
                    self.clients, response
                )

                self.llama_service.add_user_message(
                    self.messages, tool_result_parts
                )
            else:
                final_text_response = self.llama_service.text_from_message(
                    response
                )
                break

        return final_text_response
